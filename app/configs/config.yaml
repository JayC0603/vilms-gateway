host:
  platform: dgpu
  cuda-version: "12.6"

docker:
  docker-registry: docker-registry.viais.vision
  docker-project: vilms
  docker-network: qvision
  image:
    # Optional override for Ollama image repo (useful on Jetson/aarch64).
    # Example Jetson: set to your ARM64-compatible Ollama image repo, then keep docker.tag.engine as the image tag.
    # ollama: <your-jetson-ollama-image-repo>
    ollama: ollama/ollama
  tag:
    gateway: latest
    engine: latest

serving:
  # Ollama runs a single "serve" service; model is selected by the "model" field in the payload
  # CPU-safe mode: use Ollama as chat backend. (Switch back to vllm when Docker GPU is available.)
  engine: ollama

  # Primary chat backend URL (backward compatible)
  # Use localhost when running gateway directly from IDE/host machine.
  # Use http://vilms-ollama:11434/... only when gateway runs inside Docker network.
  base-url: http://vilms-ollama:11434/v1/chat/completions
  # Optional split URLs (kept for future GPU/vLLM setup):
  vllm-base-url: http://vilms-vllm:8000
  ollama-base-url: http://vilms-ollama:11434/v1/chat/completions
  models:
    # ===== Preferred combo (Qwen3) =====
    - name: qwen3:4b-instruct
      type: llm
      # Optional per-model routing override (used by gateway when mixed backends are available)
      # engine: vllm
      aliases: [LLM_SMALL]
      params:
        - temperature: 0.7
        - max-tokens: 1024
        - stream: false
      # Optional path for spaw.sh when generating vLLM services in engine=vllm mode
      # path: models/hf/Qwen3-4B-Instruct

    - name: qwen3-vl:4b-instruct
      type: vlm
      # engine: ollama
      aliases: [VLM_SMALL]
      params:
        - max-frames: 8
        - temperature: 0.7
        - max-tokens: 1024
        - stream: false
  # Gateway-side payload optimizer also uses this on Jetson (host.platform=js)
  # to keep only the latest N image frames in OpenAI-style vision requests.
  default-max-frames: 8

# Embeddings are separate from chat completions
embedding:
  # Jetson bring-up recommendation: set false first, then enable if resource budget allows.
  enabled: true
  # HF model name (if you run embeddings via transformers/sentence-transformers)
  model: Qwen/Qwen3-Embedding-4B

  # If you run a separate embedding service, enable this and point the gateway to it:
  # base-url: http://vilms-embedding:8001/v1/embeddings

model-aliases:
  LLM: Qwen3-4B-Instruct
  VLM: Qwen3VL-4B-Instruct
  Embedding: Qwen3-Embedding-4B
  Embbeding: Qwen3-Embbeding-4B
  llm: Qwen3-4B-Instruct
  vlm: Qwen3VL-4B-Instruct
  embedding: Qwen3-Embedding-4B
  embbeding: Qwen3-Embbeding-4B
  qwen3-4b-instruct: qwen3:4b-instruct
  qwen3vl-4b-instruct: qwen3-vl:4b-instruct
  qwen3-embedding-4b: Qwen/Qwen3-Embedding-4B
  qwen3-embbeding-4b: Qwen/Qwen3-Embedding-4B
  Qwen3-4B-Instruct: qwen3:4b-instruct
  Qwen3VL-4B-Instruct: qwen3-vl:4b-instruct
  Qwen3-Embedding-4B: Qwen/Qwen3-Embedding-4B
  Qwen3-Embbeding-4B: Qwen/Qwen3-Embedding-4B
